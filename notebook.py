# -*- coding: utf-8 -*-
"""Notebook.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12FNApObJn_W969hPhBv5YbuA4djjKwpo

## Task 1: Import Libraries and Modules
"""

# Write the code to import the required libraries here.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import tensorflow as tf

# Write the code to import the required modules here.

from tensorflow.keras.preprocessing import sequence
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

"""## Task 2: Load the Datasets"""

# Write the code to load the datasets here.
df_train = pd.read_csv("./usercode/train.csv")
df_test = pd.read_csv("./usercode/test.csv")

# Write the code to print the first five records of training set here.
df_train.head()

# Write the code to print the first five records of test set here.

df_test.head()

"""## Task 3: Check the Missing Values"""

# Write the code to check for missing values of training set here.

df_train.isnull().sum()

# Write the code to check for missing values of test set here.
df_test.isnull().sum()

# Write the code to remove the rows with missing values here.

df_train = df_train.dropna()
df_test = df_test.dropna()

"""## Task 4: Check the Target Balance"""

# Write the code to check the target balance here.

unique_values = df_train["category"].unique()
value_counts = df_train["category"].value_counts()
print(unique_values)
print(value_counts)

"""## Task 5: Display the Distribution of Labeled Intents"""

# Write the code to display the distribution here.

sns.set(rc={"figure.figsize":(10, 15)})
sns.countplot(data=df_train, y="category", order=df_train['category'].value_counts().index)
plt.show()

"""## Task 6: Shuffle the Dataset"""

# Write the code to shuffle dataset here.

df_train = df_train.sample(frac = 1)

"""## Task 7: Transform the Data"""

# Write the code to transform data here.

train_data = df_train['text'].to_numpy()
test_data = df_test['text'].to_numpy()

"""## Task 8: Tokenize the Words"""

# Write the code to set the configuration parameters for tokenization here.
vocab_size = 5000
oov_tok = '<OOV>'

# Write the code to use TensorFlow’s Tokenizer to convert words to integers here.

tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)
tokenizer.fit_on_texts(train_data)

# Write the code to convert each text into a sequence of integers here.

train_sequences = tokenizer.texts_to_sequences(train_data)
test_sequences = tokenizer.texts_to_sequences(test_data)

"""## Task 9: Pad the Training and Test Sequences"""

# Write the code to set the configuration parameter for sequence padding here.

max_length = 50
trunc_type = 'post'
padding_type = 'post'

# Write the code to perform sequence padding here.

x_train = pad_sequences(train_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)

x_test = pad_sequences(test_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)

"""## Task 10: Encode the Labels"""

# Write the code to convert the category column of training and test datasets here.

train_labels = pd.Categorical(df_train['category']).codes
test_labels = pd.Categorical(df_test['category']).codes

# Write the code to reshape the array of encoded labels into a two-dimensional array here.

y_train = train_labels.reshape((10003,1))
y_test = test_labels.reshape((3080,1))

"""## Task 11: Prepare a Validation Set"""

# Write the code to prepare validation set here.

partial_x_train = x_train[:9000]
partial_y_train = y_train[:9000]

x_val = x_train[9000:]
y_val = y_train[9000:]

"""## Task 12: Define a Neural Network Architecture"""

# Write the code to define neural network architecture here.

embedding_dim = 64

model = tf.keras.Sequential([
    tf.keras.layers.Embedding(vocab_size, embedding_dim),
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(embedding_dim)),
    tf.keras.layers.Dense(embedding_dim, activation='relu'),
    tf.keras.layers.Dense(77, activation='softmax')
])

model.summary()

"""## Task 13: Fit the Model"""

# Write the code to set the training configuration here.

model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# Write the code to fit the model with the partial training dataset here.

num_epochs = 20

history = model.fit(partial_x_train, partial_y_train, epochs=num_epochs, validation_data=(x_val, y_val), verbose=2)

"""## Task 14: Plot Training and Validation Loss Curves"""

# Write the code to convert the training history to a pandas DataFrame here.
df_history = pd.DataFrame(history.history)
df_history.rename(columns ={"loss": "train_loss"}, inplace=True)

# Write the code to plot training and validation loss curves here.
sns.set(rc={"figure.figsize":(8, 6)})
my_plot = sns.lineplot(data=df_history[["train_loss","val_loss"]])
my_plot.set_xlabel('Epochs')
my_plot.set_ylabel("Loss")
my_plot.set_title("Training and Validation Loss")
plt.show()

"""## Task 15: Retrain the Model"""

# Write the code to retrain model here.
# Epoch number may differ from your observation as the randomized training data.
num_epochs = 14

model = tf.keras.Sequential([
 tf.keras.layers.Embedding(vocab_size, embedding_dim),
 tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(embedding_dim)),
 tf.keras.layers.Dense(embedding_dim, activation='relu'),
 tf.keras.layers.Dense(77, activation='softmax')
])model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

model.fit(x_train, y_train, epochs=num_epochs, verbose=2)

"""## Task 16: Examine the Model’s Performance with the Test Dataset"""

# Write the code to examine model's performance with test dataset here.
results = model.evaluate(x_test, y_test)

"""## Task 17: Predict the Outcomes for New Data"""

# Write the code to convert the query text samples into padded sequences here.
input_text = ["I am still waiting for my card, when will it arrive?",
                    "Which fiat currency do you support?",
                    "Help, I just lost my card!"]

input_text_arr = np.array(input_text)
input_text_sequences = tokenizer.texts_to_sequences(input_text_arr)
input_text_padded = pad_sequences(input_text_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)

# Write the code to use the trained model to predict the outcomes here.
predictions = model.predict(input_text_padded)
predicted_classes = np.argmax(predictions,axis=1)

"""## Task 18: Match Predictions with Severity Levels"""

# Write the code to convert the prediction to the original category text here.
df_train['category_codes'] = pd.Categorical(df_train['category']).codes
df_ref = df_train[["category", "category_codes"]]
df_ref = df_ref.drop_duplicates()
df_ref = df_ref.reset_index(drop=True)

predicted_categories = []

for x in predicted_classes:
  target_code = x
  category = df_ref[df_ref["category_codes"] == target_code]["category"].values[0]
  predicted_categories.append(category)

# Write the code to match each prediction to the corresponding severity level here.

df_severity = pd.read_csv("./usercode/severity_levels.csv")

for y in predicted_categories:
  target_index = df_severity [df_severity ["category"]==y].index.values[0]
  level = df_severity._get_value(target_index, 'severity_level')
  print("Predicted Category: " + y)
  print("Severity Level: " + level )
  print("\n")